# use cases:
# - simulate a captive portal
# - replicate a WPA enterprise AP

# clone a web-page
wget \
    --mirror \ # browse all the directories of the website
    --page-requisites \ # get all assets/elements (CSS/JS/images)
    --adjust-extension \ # save files with .html on the end
    --span-hosts \ # include necessary assets from offsite as well
    --convert-links \ # update links to still work in the local version
    --restrict-file-names windows \ # modify filenames to work in Windows as well
    --domains example.com \ # do not follow links outside this domain
    --no-parent \ # do not follow links outside the directory you pass in
    --wait 1 \ # delay the requests
    --random-wait \ # randomize the requests by a ratio of the wait arg
    --execute robots=off \ # ignore the directives in robots.txt
    --no-cookies \ # disable tracking cookies
    --output-file cloning.log \ # keep a trace of the processing
    http://www.example.com # The URL to download

# shortened
wget -rNl inf -p -E -H -k -D example.com -np -w 1 -e robots=off --no-cookies --random-wait -o cloning.log http://www.example.com

# using httrack
httrack "http://www.all.net/" -O "/tmp/www.all.net" -v

# update a list of websites
httrack -%L linkfile -O /tmp/shoesizes -B --update

# filter files larger than 1024 kB
httrack "http://example.com/" -O "/tmp/example.com" "-*[>1024]" -v

# filter media files
httrack "http://example.com/" -O "/tmp/example.com" "-mime:video/*" "-mime:audio/*" -v

# limit the crawling to the original domain
httrack --ext-depth=0 --stay-on-same-address --path "./example.com" "http://example.com"

# other useful options
httrack "http://example.com" \
  --path "./example.com" \
  --near \
  --socket=8 \
  --stay-on-same-address \
  --ext-depth=0 \
